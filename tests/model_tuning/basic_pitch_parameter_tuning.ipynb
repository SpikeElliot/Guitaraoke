{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Coremltools is not installed. If you plan to use a CoreML Saved Model, reinstall basic-pitch with `pip install 'basic-pitch[coreml]'`\n",
      "WARNING:root:tflite-runtime is not installed. If you plan to use a TFLite Model, reinstall basic-pitch with `pip install 'basic-pitch tflite-runtime'` or `pip install 'basic-pitch[tf]'\n",
      "WARNING:root:onnxruntime is not installed. If you plan to use an ONNX Model, reinstall basic-pitch with `pip install 'basic-pitch[onnx]'`\n",
      "WARNING:tensorflow:From c:\\Users\\spike\\anaconda3\\envs\\Project2\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from basic_pitch.inference import Model, predict\n",
    "from basic_pitch import ICASSP_2022_MODEL_PATH\n",
    "import numpy as np\n",
    "import py_midicsv as pm\n",
    "import mir_eval\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "# Preload the Basic Pitch model\n",
    "model = Model(ICASSP_2022_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0aff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demucs.separate\n",
    "\n",
    "# Perform demucs guitar separation on test song\n",
    "separation_args = [\n",
    "    \"--two-stems\", \"guitar\"\n",
    "    \"-n\", \"htdemucs_6s\"\n",
    "    \"-o\", \"./train\"\n",
    "    \"-d\", \"cuda\"\n",
    "    \"--float32\",\n",
    "    \"./tracks/test-song.wav\"\n",
    "]\n",
    "demucs.separate.main(separation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a327296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_note_events(note_events: list) -> tuple[np.ndarray, np.ndarray]:\n",
    "    events = []\n",
    "    for e in note_events:\n",
    "        # Intervals need to positive so set offset to arbitrary\n",
    "        # value higher than onset as offsets will be ignored regardless\n",
    "        events.append([e[0], e[0]+1e-6, e[2]])\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        events, columns=[\"note_on\", \"note_off\", \"midi_pitch\"]\n",
    "    ).sort_values(\"note_on\")\n",
    "\n",
    "    intervals = df[[\"note_on\", \"note_off\"]].to_numpy()\n",
    "    \n",
    "    # mir_eval needs pitches in Hz\n",
    "    pitches = librosa.midi_to_hz(df[\"midi_pitch\"].to_numpy())\n",
    "\n",
    "    return intervals, pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc11d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_note_events(csv_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Read saved CSV as a DataFrane\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        sep=None,\n",
    "        engine=\"python\",\n",
    "        index_col=False\n",
    "    )\n",
    "\n",
    "    # Find time and tempo info to get song note_on times in seconds\n",
    "    # to allow comparison of predicted notes and \"true\" notes\n",
    "    PPQ = int(df.columns[5])\n",
    "    TEMPO = df.loc[df[\" Header\"] == \" Tempo\"][df.columns[3]].iloc[0]\n",
    "    BPM = 60_000_000 / int(TEMPO)\n",
    "    TICK_IN_S = 60 / (BPM * PPQ)\n",
    "\n",
    "    df.drop(df.columns[[0,3,5]], axis=1, inplace=True) # Drop unnecessary cols\n",
    "\n",
    "    note_on_events = df.loc[df[\" Header\"] == \" Note_on_c\"].copy().sort_values(\" 0\")\n",
    "    note_on_events[\" 0\"] *= TICK_IN_S # Convert times to seconds\n",
    "\n",
    "    ons = note_on_events[\" 0\"].to_numpy()\n",
    "    offs = (note_on_events[\" 0\"] + 1e-6).to_numpy() # Fake offsets (will not be used)\n",
    "    intervals = np.stack((ons, offs), axis=1) # Create intervals ndarray\n",
    "\n",
    "    # mir_eval needs pitches in Hz\n",
    "    pitches = librosa.midi_to_hz(note_on_events[\" 3\"].to_numpy().astype(int))\n",
    "\n",
    "    return intervals, pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d424dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_events = predict( # Get note events from Basic Pitch prediction\n",
    "    \"./train/htdemucs_6s/test-song/guitar.wav\",\n",
    "    model_or_model_path=model,\n",
    ")[2]\n",
    "\n",
    "pred_intervals, pred_pitches = preprocess_note_events(note_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b889af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the tab-converted midi file as a CSV file\n",
    "csv_string_list = pm.midi_to_csv(\"./test/test-song.mid\")\n",
    "\n",
    "# Write the CSV file\n",
    "with open(\"test-song.csv\", \"w\") as f:\n",
    "    f.writelines(csv_string_list)\n",
    "\n",
    "test_intervals, test_pitches = load_test_note_events(\"test-song.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the annotations before evaluating predictions\n",
    "mir_eval.transcription.validate(\n",
    "    test_intervals, test_pitches,\n",
    "    pred_intervals, pred_pitches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f46563dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.10174855605665005\n",
      "Precision: 0.15269532177630016\n",
      "Recall: 0.07629330802088277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the classification metrics for the predicted notes against the ground truth tabbed notes\n",
    "precision, recall, f1, avg_overlap_ratio = mir_eval.transcription.precision_recall_f1_overlap(\n",
    "    test_intervals, test_pitches,\n",
    "    pred_intervals, pred_pitches,\n",
    "    offset_ratio=None # Ignore note offsets\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "F1 Score: {f1}\n",
    "Precision: {precision}\n",
    "Recall: {recall}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662f25f",
   "metadata": {},
   "source": [
    "The default model setup achieved an F1 score of **~0.10**.\n",
    "\n",
    "The model's poor performance on the test song could be caused by factors such as:\n",
    "\n",
    "- Bleed and noise from guitar separation negatively impacting predictions\n",
    "- The ground truth (guitar tab converted to MIDI file) is inaccurate\n",
    "- The model's training data was not representative enough of certain genres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
