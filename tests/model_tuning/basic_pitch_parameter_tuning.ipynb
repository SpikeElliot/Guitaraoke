{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdcde98",
   "metadata": {},
   "source": [
    "# Basic Pitch Parameter Tuning for Optimal Guitar Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7e0ab",
   "metadata": {},
   "source": [
    "## Setup and Preloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c4ae4",
   "metadata": {},
   "source": [
    "Necessary libraries are imported and the default Basic Pitch model is preloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd20b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mir_eval\n",
    "import librosa\n",
    "from basic_pitch.inference import Model, predict\n",
    "from basic_pitch import ICASSP_2022_MODEL_PATH\n",
    "\n",
    "# Preload the Basic Pitch model\n",
    "MODEL = Model(ICASSP_2022_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5c62c",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d4c6a",
   "metadata": {},
   "source": [
    "The IDMT-SMT-Guitar Dataset's \"Dataset 2\", containing various monophonic and polyphonic guitar recordings is split into a train and test set at a 80:20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbda433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path(\"./audio\")\n",
    "ANNOTATION_DIR = Path(\"./annotation\")\n",
    "\n",
    "# Shuffle the data for random split\n",
    "ITEMS = np.stack(\n",
    "    (os.listdir(AUDIO_DIR), os.listdir(ANNOTATION_DIR)),\n",
    "    axis=1\n",
    ")\n",
    "np.random.shuffle(ITEMS)\n",
    "\n",
    "DATASET_SIZE = len(os.listdir(AUDIO_DIR))\n",
    "TEST_SIZE = int(DATASET_SIZE * 0.2)\n",
    "\n",
    "# Move random 20% of data to test set\n",
    "for files in ITEMS[:TEST_SIZE]:\n",
    "    shutil.move(AUDIO_DIR / files[0], \"./test/audio\")\n",
    "    shutil.move(ANNOTATION_DIR / files[1], \"./test/annotation\")\n",
    "\n",
    "# Move the remaining 80% to train set\n",
    "for files in ITEMS[TEST_SIZE:]:\n",
    "    shutil.move(AUDIO_DIR / files[0], \"./train/audio\")\n",
    "    shutil.move(ANNOTATION_DIR / files[1], \"./train/annotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770bd81a",
   "metadata": {},
   "source": [
    "## Data Formatting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc5738",
   "metadata": {},
   "source": [
    "Functions are defined below that perform necessary postprocessing steps to correctly format the note events for use with the `mir_eval` library.\n",
    "\n",
    "##### **pred_note_events()**\n",
    "\n",
    "- Takes a resultant `note_events` list from the Basic Pitch model's `predict()` method and returns predicted intervals and pitches ndarrays.\n",
    "\n",
    "##### **true_note_events()**\n",
    "\n",
    "- Parses an \"annotation\" XML file from a provided path, returning true intervals and pitches ndarrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5ca59",
   "metadata": {},
   "source": [
    "Note intervals need to be in the format of a 2D ndarray, and need to be positive (offset > onset) for `mir_eval`. Therefore, offsets are set to an arbitrary value (onset + 1e-6) as they will be discounted during evaluation.\n",
    "\n",
    "Pitches also need to be in Hz, so Librosa's `midi_to_hz()` method will be used to convert from MIDI notes to frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a327296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_note_events(note_events: list) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get intervals and pitches ndarrays from a Basic Pitch predicted note_events\n",
    "    list.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for e in note_events:\n",
    "        events.append([e[0], e[0]+1e-6, e[2]])\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        events, columns=[\"note_on\", \"note_off\", \"midi_pitch\"]\n",
    "    ).sort_values(\"note_on\")\n",
    "\n",
    "    # Prepare ndarrays for mir_eval\n",
    "    intervals = df[[\"note_on\", \"note_off\"]].to_numpy()\n",
    "    pitches = librosa.midi_to_hz(df[\"midi_pitch\"].to_numpy())\n",
    "\n",
    "    return intervals, pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8573a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_note_events(xml_path: str | Path) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parse an annotation XML file given its path and return resultant intervals \n",
    "    and pitches ndarrays.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    intervals = []\n",
    "    pitches = []\n",
    "    for event in root.findall(\"./transcription/event\"):\n",
    "        for child in event:\n",
    "            if child.tag == \"onsetSec\":\n",
    "                onset = float(child.text)\n",
    "                intervals.append([onset, onset+1e-6]) # Onset and placeholder offset\n",
    "            elif child.tag == \"pitch\":\n",
    "                pitches.append(int(child.text)) # MIDI note\n",
    "\n",
    "    # Prepare ndarrays for mir_eval\n",
    "    intervals = np.array(intervals)\n",
    "    pitches = librosa.midi_to_hz(np.array(pitches))\n",
    "\n",
    "    return intervals, pitches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce69e66",
   "metadata": {},
   "source": [
    "## Random Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f2b79",
   "metadata": {},
   "source": [
    "- Takes the mean F1, Recall and Precision scores for a number, `iterations`, of random parameter setups chosen from a given parameter distributions\n",
    "dictionary `params`. \n",
    "\n",
    "- A results list containing scores and parameter configurations for each iteration is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(params: dict[str], iterations: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Get model note prediction scores on all training data using a random\n",
    "    parameter setup chosen from a provided parameter distributions\n",
    "    dictionary for a given number of iterations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Get mean performance scores for 10 random parameter setups\n",
    "    for _ in range(iterations):\n",
    "        f1_scores = []\n",
    "        recall_scores = []\n",
    "        precision_scores = []\n",
    "\n",
    "        param_setup = { # Randomly select from param distributions\n",
    "            \"onset_threshold\": np.random.choice(params[\"onset_threshold\"]),\n",
    "            \"frame_threshold\": np.random.choice(params[\"frame_threshold\"]),\n",
    "            \"minimum_note_length\": np.random.choice(params[\"minimum_note_length\"]),\n",
    "            \"minimum_frequency\": np.random.choice(params[\"minimum_frequency\"]),\n",
    "            \"maximum_frequency\": np.random.choice(params[\"maximum_frequency\"]),\n",
    "            \"multiple_pitch_bends\": np.random.choice(params[\"multiple_pitch_bends\"]),\n",
    "            \"melodia_trick\": np.random.choice(params[\"melodia_trick\"])\n",
    "        }\n",
    "\n",
    "        # Make predictions on all training data and save scores\n",
    "        for i, audio_file in enumerate(os.listdir(\"./train/audio\")):\n",
    "            note_events = predict(\n",
    "                audio_path=f\"./train/audio/{audio_file}\",\n",
    "                model_or_model_path=MODEL,\n",
    "                onset_threshold=param_setup[\"onset_threshold\"],\n",
    "                frame_threshold=param_setup[\"frame_threshold\"],\n",
    "                minimum_note_length=param_setup[\"minimum_note_length\"],\n",
    "                minimum_frequency=param_setup[\"minimum_frequency\"],\n",
    "                maximum_frequency=param_setup[\"maximum_frequency\"],\n",
    "                multiple_pitch_bends=param_setup[\"multiple_pitch_bends\"],\n",
    "                melodia_trick=param_setup[\"melodia_trick\"]\n",
    "            )[2]\n",
    "            pred_intervals, pred_pitches = pred_note_events(note_events)\n",
    "\n",
    "            annotation_files = os.listdir(\"./train/annotation\")\n",
    "            true_intervals, true_pitches = true_note_events(\n",
    "                f\"./train/annotation/{annotation_files[i]}\"\n",
    "            )\n",
    "\n",
    "            scores = mir_eval.transcription.precision_recall_f1_overlap(\n",
    "                true_intervals, true_pitches,\n",
    "                pred_intervals, pred_pitches,\n",
    "                offset_ratio=None # Ignore note offsets\n",
    "            )\n",
    "            precision_scores.append(scores[0])\n",
    "            recall_scores.append(scores[1])\n",
    "            f1_scores.append(scores[2])\n",
    "\n",
    "        # Add parameter setup and mean scores to results list\n",
    "        results.append({\n",
    "            \"param_setup\": param_setup,\n",
    "            \"mean_f1_score\": np.mean(f1_scores),\n",
    "            \"mean_recall_score\": np.mean(recall_scores),\n",
    "            \"mean_precision_score\": np.mean(precision_scores),\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1938b36",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"onset_threshold\": np.linspace(0.1, 0.9),\n",
    "    \"frame_threshold\": np.linspace(0.1, 0.9),\n",
    "    \"minimum_note_length\": np.linspace(70, 140),\n",
    "    \"minimum_frequency\": [None],\n",
    "    \"maximum_frequency\": [None],\n",
    "    \"multiple_pitch_bends\": [True, False],\n",
    "    \"melodia_trick\": [True, False]\n",
    "}\n",
    "\n",
    "random_search(param_distributions, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4419e",
   "metadata": {},
   "source": [
    "### Preliminary Best Parameter Setup:\n",
    "\n",
    "- onset_threshold: **0.753061224489796**\n",
    "\n",
    "- frame_threshold: **0.42653061224489797**\n",
    "\n",
    "- minimum_note_length: **118.57142857142857**\n",
    "\n",
    "- minimum_frequency: **None**\n",
    "\n",
    "- maximum_frequency: **None**\n",
    "\n",
    "- multiple_pitch_bends: **True**\n",
    "\n",
    "- melodia_trick: **False**\n",
    "\n",
    "### Best Setup Scores:\n",
    "\n",
    "- mean_f1_score: **0.770**\n",
    "\n",
    "- mean_recall_score: **0.778**\n",
    "\n",
    "- mean_precision_score: **0.798**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
